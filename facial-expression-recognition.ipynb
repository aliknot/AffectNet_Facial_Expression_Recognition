{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7595888,"sourceType":"datasetVersion","datasetId":4420036}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial Expression Recognition","metadata":{}},{"cell_type":"markdown","source":"## Import Essential Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport shutil\nimport math\nfrom PIL import Image\nimport cv2\nimport tensorflow as tf\nimport random\nimport pandas as pd\nimport seaborn as sns\n\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T14:42:07.160326Z","iopub.execute_input":"2025-07-03T14:42:07.160592Z","iopub.status.idle":"2025-07-03T14:42:07.165407Z","shell.execute_reply.started":"2025-07-03T14:42:07.160574Z","shell.execute_reply":"2025-07-03T14:42:07.164530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Seed","metadata":{}},{"cell_type":"code","source":"# Set all random seeds (Python, NumPy, and TensorFlow)\ntf.keras.utils.set_random_seed(42)\n\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:51.702997Z","iopub.execute_input":"2025-07-03T12:38:51.703495Z","iopub.status.idle":"2025-07-03T12:38:51.781873Z","shell.execute_reply.started":"2025-07-03T12:38:51.703476Z","shell.execute_reply":"2025-07-03T12:38:51.781245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config Tensorflow to use GPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# List all physical devices (CPUs and GPUs) that TensorFlow can see\nphysical_devices = tf.config.list_physical_devices()\nprint(f\"Physical devices detected: {physical_devices}\")\n\n# Specifically list GPUs\ngpu_devices = tf.config.list_physical_devices('GPU')\nif gpu_devices:\n    print(f\"\\nNumber of GPUs available: {len(gpu_devices)}\")\n    for i, gpu in enumerate(gpu_devices):\n        print(f\"  GPU {i}: {gpu}\")\n    print(\"\\nTensorFlow will automatically use the GPU if available.\")\nelse:\n    print(\"\\nNo GPU devices found. TensorFlow will run on CPU.\")\n\n# You can also check if a random tensor is placed on GPU by default\n# This should show GPU if one is available and being used\ntest_tensor = tf.constant([1.0, 2.0, 3.0])\nprint(f\"\\nDefault device for a tensor: {test_tensor.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:51.782608Z","iopub.execute_input":"2025-07-03T12:38:51.782873Z","iopub.status.idle":"2025-07-03T12:38:54.411841Z","shell.execute_reply.started":"2025-07-03T12:38:51.782850Z","shell.execute_reply":"2025-07-03T12:38:54.411033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Global Variables","metadata":{}},{"cell_type":"code","source":"# List of emotions\nemotions = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:54.413605Z","iopub.execute_input":"2025-07-03T12:38:54.414063Z","iopub.status.idle":"2025-07-03T12:38:54.417237Z","shell.execute_reply.started":"2025-07-03T12:38:54.414045Z","shell.execute_reply":"2025-07-03T12:38:54.416520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of emotion lables\nemotion_labels = {\n    0: 'anger',\n    1: 'contempt',\n    2: 'disgust',\n    3: 'fear',\n    4: 'happy',\n    5: 'neutral',\n    6: 'sad',\n    7: 'surprise'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:54.417970Z","iopub.execute_input":"2025-07-03T12:38:54.418269Z","iopub.status.idle":"2025-07-03T12:38:54.434861Z","shell.execute_reply.started":"2025-07-03T12:38:54.418244Z","shell.execute_reply":"2025-07-03T12:38:54.434346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Global URLs","metadata":{}},{"cell_type":"code","source":"# Input related URLs\ninput_base_url = '/kaggle/input/'\n\ntrain_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/train/images')\nvalid_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/valid/images')\ntest_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/test/images')\n\ntrain_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/train/labels')\nvalid_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/valid/labels')\ntest_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/test/labels')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:54.435535Z","iopub.execute_input":"2025-07-03T12:38:54.435777Z","iopub.status.idle":"2025-07-03T12:38:54.456193Z","shell.execute_reply.started":"2025-07-03T12:38:54.435761Z","shell.execute_reply":"2025-07-03T12:38:54.455686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output related URLs\noutput_base_url = '/kaggle/working/'\n\norganized_base_dir = os.path.join(output_base_url, 'organized_images')\n\norganized_train_images = os.path.join(organized_base_dir, 'train')\norganized_valid_images = os.path.join(organized_base_dir, 'valid')\norganized_test_images = os.path.join(organized_base_dir, 'test')\n\nresized_base_dir = os.path.join(output_base_url, 'resized_images')\n\nresized_train_images = os.path.join(resized_base_dir, 'train')\nresized_valid_images = os.path.join(resized_base_dir, 'valid')\nresized_test_images = os.path.join(resized_base_dir, 'test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:08:58.356693Z","iopub.execute_input":"2025-07-03T13:08:58.357393Z","iopub.status.idle":"2025-07-03T13:08:58.362169Z","shell.execute_reply.started":"2025-07-03T13:08:58.357368Z","shell.execute_reply":"2025-07-03T13:08:58.361295Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"# Create directories\nos.makedirs(organized_base_dir, exist_ok=True)\nos.makedirs(resized_base_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:08:59.056207Z","iopub.execute_input":"2025-07-03T13:08:59.056841Z","iopub.status.idle":"2025-07-03T13:08:59.060934Z","shell.execute_reply.started":"2025-07-03T13:08:59.056813Z","shell.execute_reply":"2025-07-03T13:08:59.060103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Images into Emotion Folders","metadata":{}},{"cell_type":"code","source":"def reorganize_dataset(source_images_dir, source_labels_dir, destination_base_dir, emotion_map):\n    \"\"\"\n    Reorganizes image files into emotion-specific subfolders based on YOLO-format label files.\n\n    Args:\n        source_images_dir (str): Path to the directory containing image files (e.g., train/images).\n        source_labels_dir (str): Path to the directory containing label .txt files (e.g., train/labels).\n        destination_base_dir (str): Path to the root directory where reorganized data will be saved.\n                                    (e.g., /kaggle/working/processed_train)\n        emotion_map (dict): A dictionary mapping integer class IDs to emotion names (e.g., {0: 'anger'}).\n    \"\"\"\n    print(f\"--- Reorganizing: {source_images_dir.split('/')[-2]} set ---\")\n\n    # Create destination directories for each emotion\n    for emotion_id, emotion_name in emotion_map.items():\n        # Using the emotion name (e.g., 'anger') as the subfolder name\n        class_folder = os.path.join(destination_base_dir, emotion_name)\n        os.makedirs(class_folder, exist_ok=True) # exist_ok=True prevents error if folder already exists\n        print(f\"  Created directory: {class_folder}\")\n\n    # Iterate through each image file\n    image_files = [f for f in os.listdir(source_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    print(f\"  Found {len(image_files)} image files in {source_images_dir}\")\n\n    processed_count = 0\n    skipped_count = 0\n\n    for img_filename in image_files:\n        img_path = os.path.join(source_images_dir, img_filename)\n        \n        # Construct the corresponding label file name\n        # Remove the image extension (.png, .jpg) and replace with .txt\n        base_filename = os.path.splitext(img_filename)[0]\n        label_filename = base_filename + '.txt'\n        label_path = os.path.join(source_labels_dir, label_filename)\n\n        if not os.path.exists(label_path):\n            print(f\"    Warning: Label file not found for {img_filename} at {label_path}. Skipping.\")\n            skipped_count += 1\n            continue\n\n        try:\n            with open(label_path, 'r') as f:\n                # Read the first line (assuming one object/emotion per image)\n                label_line = f.readline().strip()\n                # Extract the class_id (first number)\n                class_id = int(label_line.split(' ')[0])\n\n            emotion_name = emotion_map.get(class_id)\n            if emotion_name is None:\n                print(f\"    Warning: Unknown class_id {class_id} for {img_filename}. Skipping.\")\n                skipped_count += 1\n                continue\n\n            destination_folder = os.path.join(destination_base_dir, emotion_name)\n            destination_path = os.path.join(destination_folder, img_filename)\n\n            # Copy the image file\n            shutil.copy(img_path, destination_path)\n            processed_count += 1\n\n            if processed_count % 1000 == 0:\n                print(f\"    Processed {processed_count} images for {source_images_dir.split('/')[-2]} set...\")\n\n        except Exception as e:\n            print(f\"    Error processing {img_filename} or {label_filename}: {e}. Skipping.\")\n            skipped_count += 1\n            continue\n\n    print(f\"--- Finished reorganizing {source_images_dir.split('/')[-2]} set. Processed: {processed_count}, Skipped: {skipped_count} ---\")\n    print(f\"Reorganized data is in: {destination_base_dir}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:54.484317Z","iopub.execute_input":"2025-07-03T12:38:54.484826Z","iopub.status.idle":"2025-07-03T12:38:54.500910Z","shell.execute_reply.started":"2025-07-03T12:38:54.484801Z","shell.execute_reply":"2025-07-03T12:38:54.500418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reorganize the training data\nreorganize_dataset(train_images_url, train_labels_url, organized_train_images, emotion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:38:54.501515Z","iopub.execute_input":"2025-07-03T12:38:54.501708Z","iopub.status.idle":"2025-07-03T12:46:31.778725Z","shell.execute_reply.started":"2025-07-03T12:38:54.501694Z","shell.execute_reply":"2025-07-03T12:46:31.777997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(organized_valid_images, exist_ok=True)\n\n# Reorganize the validation data\nreorganize_dataset(valid_images_url, valid_labels_url, organized_valid_images, emotion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:46:31.781798Z","iopub.execute_input":"2025-07-03T12:46:31.782137Z","iopub.status.idle":"2025-07-03T12:47:56.495291Z","shell.execute_reply.started":"2025-07-03T12:46:31.782115Z","shell.execute_reply":"2025-07-03T12:47:56.494513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(organized_test_images, exist_ok=True)\n\n# Reorganize the validation data\nreorganize_dataset(test_images_url, test_labels_url, organized_test_images, emotion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:47:56.496108Z","iopub.execute_input":"2025-07-03T12:47:56.496367Z","iopub.status.idle":"2025-07-03T12:48:39.244661Z","shell.execute_reply.started":"2025-07-03T12:47:56.496345Z","shell.execute_reply":"2025-07-03T12:48:39.243931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Verify Organized Images Directories","metadata":{}},{"cell_type":"code","source":"# Define a function to count images in the reorganized structure\ndef count_images_in_organized_dirs(base_dir, emotion_list):\n    \"\"\"\n    Counts images in emotion subfolders within a given base directory.\n\n    Args:\n        base_dir (str): The root directory of the organized dataset split (e.g., '/kaggle/working/organized_images/train').\n        emotion_list (list): A list of emotion names (which are also the subfolder names).\n\n    Returns:\n        dict: A dictionary with emotion names as keys and image counts as values.\n    \"\"\"\n    counts = {}\n    total_images = 0\n    print(f\"--- Checking: {base_dir.split('/')[-1].capitalize()} Set ---\")\n\n    if not os.path.exists(base_dir):\n        print(f\"  Warning: Base directory not found: {base_dir}. Skipping.\")\n        return counts, 0\n\n    for emotion in emotion_list:\n        emotion_path = os.path.join(base_dir, emotion)\n        if os.path.exists(emotion_path) and os.path.isdir(emotion_path):\n            # Count only files (not subdirectories)\n            num_images = len([f for f in os.listdir(emotion_path) if os.path.isfile(os.path.join(emotion_path, f))])\n            counts[emotion] = num_images\n            total_images += num_images\n            print(f\"  {emotion.capitalize()}: {num_images} images\")\n        else:\n            print(f\"  Warning: Emotion directory not found for {emotion} in {base_dir}.\")\n            counts[emotion] = 0\n\n    print(f\"  Total images in {base_dir.split('/')[-1].capitalize()} Set: {total_images}\\n\")\n    return counts, total_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.245477Z","iopub.execute_input":"2025-07-03T12:48:39.245757Z","iopub.status.idle":"2025-07-03T12:48:39.253609Z","shell.execute_reply.started":"2025-07-03T12:48:39.245730Z","shell.execute_reply":"2025-07-03T12:48:39.252757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Training Set\ntrain_counts, total_train = count_images_in_organized_dirs(organized_train_images, emotions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.254415Z","iopub.execute_input":"2025-07-03T12:48:39.254739Z","iopub.status.idle":"2025-07-03T12:48:39.371420Z","shell.execute_reply.started":"2025-07-03T12:48:39.254722Z","shell.execute_reply":"2025-07-03T12:48:39.370874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Validation Set\nvalid_counts, total_valid = count_images_in_organized_dirs(organized_valid_images, emotions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.372044Z","iopub.execute_input":"2025-07-03T12:48:39.372238Z","iopub.status.idle":"2025-07-03T12:48:39.406374Z","shell.execute_reply.started":"2025-07-03T12:48:39.372225Z","shell.execute_reply":"2025-07-03T12:48:39.405870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Test Set\ntest_counts, total_test = count_images_in_organized_dirs(organized_test_images, emotions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.406912Z","iopub.execute_input":"2025-07-03T12:48:39.407100Z","iopub.status.idle":"2025-07-03T12:48:39.427423Z","shell.execute_reply.started":"2025-07-03T12:48:39.407086Z","shell.execute_reply":"2025-07-03T12:48:39.426621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- Overall Summary of Reorganized Dataset ---\")\nprint(f\"Total images across all sets: {total_train + total_valid + total_test}\")\nprint(f\"Train Set Total: {total_train}\")\nprint(f\"Valid Set Total: {total_valid}\")\nprint(f\"Test Set Total: {total_test}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.428250Z","iopub.execute_input":"2025-07-03T12:48:39.428546Z","iopub.status.idle":"2025-07-03T12:48:39.432508Z","shell.execute_reply.started":"2025-07-03T12:48:39.428524Z","shell.execute_reply":"2025-07-03T12:48:39.432026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store counts in global variables for future use (e.g., plotting in the next step)\nglobals()['train_counts'] = train_counts\nglobals()['valid_counts'] = valid_counts\nglobals()['test_counts'] = test_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.433141Z","iopub.execute_input":"2025-07-03T12:48:39.433360Z","iopub.status.idle":"2025-07-03T12:48:39.457123Z","shell.execute_reply.started":"2025-07-03T12:48:39.433341Z","shell.execute_reply":"2025-07-03T12:48:39.456483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Class Distribution of Train, Valid and Test","metadata":{}},{"cell_type":"code","source":"print(\"Preparing DataFrames for class distribution plots...\")\n\n# Create DataFrames for each split\ntrain_df = pd.DataFrame(list(train_counts.items()), columns=['Emotion', 'Count'])\ntrain_df['Set'] = 'Train' # Add a 'Set' column (useful if we wanted a combined plot later)\n\nvalid_df = pd.DataFrame(list(valid_counts.items()), columns=['Emotion', 'Count'])\nvalid_df['Set'] = 'Validation'\n\ntest_df = pd.DataFrame(list(test_counts.items()), columns=['Emotion', 'Count'])\ntest_df['Set'] = 'Test'\n\nprint(\"DataFrames (train_df, valid_df, test_df) created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.458073Z","iopub.execute_input":"2025-07-03T12:48:39.458561Z","iopub.status.idle":"2025-07-03T12:48:39.513627Z","shell.execute_reply.started":"2025-07-03T12:48:39.458542Z","shell.execute_reply":"2025-07-03T12:48:39.513112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_class_distribution(df, title):\n    \"\"\"\n    Plots the class distribution for a given dataset split.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing 'Emotion' and 'Count' columns.\n        title (str): The title for the plot.\n    \"\"\"\n    plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n    sns.barplot(x='Emotion', y='Count', data=df, palette='viridis')\n    plt.title(title, fontsize=16)\n    plt.xlabel('Emotion', fontsize=12)\n    plt.ylabel('Number of Images', fontsize=12)\n    plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate labels for readability\n    plt.yticks(fontsize=10)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout() # Adjust layout to prevent elements from overlapping\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.514249Z","iopub.execute_input":"2025-07-03T12:48:39.514433Z","iopub.status.idle":"2025-07-03T12:48:39.519262Z","shell.execute_reply.started":"2025-07-03T12:48:39.514412Z","shell.execute_reply":"2025-07-03T12:48:39.518696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot train directory class distribution\nplot_class_distribution(train_df, 'Training Set Class Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.519891Z","iopub.execute_input":"2025-07-03T12:48:39.520146Z","iopub.status.idle":"2025-07-03T12:48:39.998115Z","shell.execute_reply.started":"2025-07-03T12:48:39.520125Z","shell.execute_reply":"2025-07-03T12:48:39.997199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot valid directory class distribution\nplot_class_distribution(valid_df, 'Validation Set Class Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:39.999050Z","iopub.execute_input":"2025-07-03T12:48:39.999313Z","iopub.status.idle":"2025-07-03T12:48:40.198592Z","shell.execute_reply.started":"2025-07-03T12:48:39.999292Z","shell.execute_reply":"2025-07-03T12:48:40.197736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot test directory class distribution\nplot_class_distribution(test_df, 'Test Set Class Distribution')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:40.199519Z","iopub.execute_input":"2025-07-03T12:48:40.199757Z","iopub.status.idle":"2025-07-03T12:48:40.395516Z","shell.execute_reply.started":"2025-07-03T12:48:40.199734Z","shell.execute_reply":"2025-07-03T12:48:40.394746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Images","metadata":{}},{"cell_type":"code","source":"def plot_emotion_samples_organized(dataset_split_base_dir: str, emotion_name: str, num_images_to_plot: int, images_per_row: int = 10):\n    \"\"\"\n    Plots a specified number of sample images for a given emotion from an organized dataset split.\n\n    Args:\n        dataset_split_base_dir (str): The root directory of the organized dataset split (e.g., organized_train_images).\n        emotion_name (str): The name of the emotion (e.g., 'happy', 'sad'). This should match the subfolder name.\n        num_images_to_plot (int): The maximum number of images to plot for this emotion.\n        images_per_row (int, optional): How many images to display horizontally in each row.\n                                        Defaults to 10.\n    \"\"\"\n    # Construct the full path to the emotion's folder within the given split\n    emotion_path = os.path.join(dataset_split_base_dir, emotion_name)\n\n    # Basic input validation\n    if not os.path.exists(emotion_path):\n        print(f\"Error: Directory not found for '{emotion_name}' at '{emotion_path}'. Skipping plot.\")\n        return\n\n    # Get all image files in the emotion directory\n    image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n\n    if not image_files:\n        print(f\"No images found in '{emotion_path}' for emotion '{emotion_name}'. Skipping plot.\")\n        return\n\n    # Randomly select images to plot\n    images_to_display = random.sample(image_files, min(len(image_files), num_images_to_plot))\n\n    # Calculate grid dimensions\n    num_images_actual = len(images_to_display)\n    if num_images_actual == 0:\n        print(f\"No images selected for display for '{emotion_name}'. Skipping plot.\")\n        return\n\n    num_rows = math.ceil(num_images_actual / images_per_row)\n\n    # Set up the figure and subplots\n    fig_width = images_per_row * 1.5 # Adjusted for 96x96 images\n    fig_height = num_rows * 1.8 # Slightly more height per row\n    \n    plt.figure(figsize=(fig_width, fig_height))\n    \n    # Extract the split name (e.g., 'train', 'valid', 'test') for the super title\n    split_name = dataset_split_base_dir.split(os.sep)[-1].capitalize() \n    plt.suptitle(f\"Sample Images: {emotion_name.capitalize()} ({split_name} Set)\", fontsize=18, y=1.02)\n\n    for i, img_filename in enumerate(images_to_display):\n        img_path = os.path.join(emotion_path, img_filename)\n\n        try:\n            img = Image.open(img_path)\n            \n            ax = plt.subplot(num_rows, images_per_row, i + 1)\n            ax.imshow(img)\n            ax.set_title(f\"#{i+1}\", fontsize=8) # Small title for image number\n            ax.axis('off') # Hide axes\n\n        except Exception as e:\n            print(f\"  Warning: Could not load image '{img_filename}' from '{emotion_path}': {e}. Skipping.\")\n            ax = plt.subplot(num_rows, images_per_row, i + 1) # Maintain grid even on error\n            ax.text(0.5, 0.5, 'Error', ha='center', va='center', fontsize=12, color='red')\n            ax.axis('off')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout to make space for suptitle\n    plt.show()\n    print(f\"Finished plotting {num_images_actual} images for '{emotion_name}' in the {split_name} set.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:40:45.426727Z","iopub.execute_input":"2025-07-03T16:40:45.427066Z","iopub.status.idle":"2025-07-03T16:40:45.438720Z","shell.execute_reply.started":"2025-07-03T16:40:45.427044Z","shell.execute_reply":"2025-07-03T16:40:45.437829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Training Set Images ---\")\nNUM_IMAGES = 100\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_train_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:40:47.305621Z","iopub.execute_input":"2025-07-03T16:40:47.306154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Validation Set Images ---\")\nNUM_IMAGES = 100\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_valid_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:49:30.853812Z","iopub.execute_input":"2025-07-03T12:49:30.854060Z","iopub.status.idle":"2025-07-03T12:50:20.766582Z","shell.execute_reply.started":"2025-07-03T12:49:30.854043Z","shell.execute_reply":"2025-07-03T12:50:20.765614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Test Set Images ---\")\nNUM_IMAGES = 100\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_test_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:50:20.767499Z","iopub.execute_input":"2025-07-03T12:50:20.767731Z","iopub.status.idle":"2025-07-03T12:51:10.503015Z","shell.execute_reply.started":"2025-07-03T12:50:20.767712Z","shell.execute_reply":"2025-07-03T12:51:10.502209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resize Images to 224x224","metadata":{}},{"cell_type":"code","source":"def resize_and_save_dataset(source_base_dir, destination_base_dir, target_size=(224, 224)):\n    \"\"\"\n    Resizes all images in emotion subfolders from a source directory and saves them\n    to a new destination directory, maintaining the folder structure.\n\n    Args:\n        source_base_dir (str): Path to the root directory of the already organized dataset split\n                               (e.g., '/kaggle/working/organized_images/train').\n        destination_base_dir (str): Path to the root directory where resized images will be saved.\n                                    (e.g., '/kaggle/working/resized_images/train')\n        target_size (tuple): A tuple (width, height) for the new image size.\n    \"\"\"\n    print(f\"--- Resizing and saving images for: {source_base_dir.split(os.sep)[-1].capitalize()} set to {target_size} ---\")\n\n    if not os.path.exists(source_base_dir):\n        print(f\"  Error: Source directory not found: {source_base_dir}. Skipping.\")\n        return\n\n    # Ensure the destination base directory exists\n    os.makedirs(destination_base_dir, exist_ok=True)\n\n    # Iterate through each emotion subfolder in the source directory\n    emotions_in_source = [d for d in os.listdir(source_base_dir) if os.path.isdir(os.path.join(source_base_dir, d))]\n    \n    if not emotions_in_source:\n        print(f\"  No emotion subfolders found in {source_base_dir}. Skipping.\")\n        return\n\n    total_resized_count = 0\n    total_skipped_count = 0\n\n    for emotion_folder in emotions_in_source:\n        source_emotion_path = os.path.join(source_base_dir, emotion_folder)\n        destination_emotion_path = os.path.join(destination_base_dir, emotion_folder)\n\n        # Create the corresponding emotion subfolder in the destination\n        os.makedirs(destination_emotion_path, exist_ok=True)\n        # print(f\"  Created directory: {destination_emotion_path}\")\n\n        image_files = [f for f in os.listdir(source_emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        # print(f\"    Found {len(image_files)} images in {emotion_folder}\")\n\n        resized_count_per_emotion = 0\n        skipped_count_per_emotion = 0\n\n        for img_filename in image_files:\n            source_img_path = os.path.join(source_emotion_path, img_filename)\n            destination_img_path = os.path.join(destination_emotion_path, img_filename)\n\n            try:\n                # Open image as grayscale (mode 'L')\n                img = Image.open(source_img_path).convert('L')\n                # Resize image using LANCZOS for high-quality upscaling\n                # PIL.Image.LANCZOS is a high-quality filter suitable for downsampling and upsampling\n                resized_img = img.resize(target_size, Image.LANCZOS)\n                \n                # Save the resized image\n                resized_img.save(destination_img_path)\n                resized_count_per_emotion += 1\n                total_resized_count += 1\n                \n                if resized_count_per_emotion % 1000 == 0:\n                    print(f\"      Processed {resized_count_per_emotion} images in {emotion_folder} for {source_base_dir.split(os.sep)[-1].capitalize()} set...\")\n\n            except Exception as e:\n                print(f\"    Error resizing {img_filename} from {source_emotion_path}: {e}. Skipping.\")\n                skipped_count_per_emotion += 1\n                total_skipped_count += 1\n                continue\n        \n        # print(f\"    Finished {emotion_folder}: Resized {resized_count_per_emotion}, Skipped {skipped_count_per_emotion}\")\n\n    print(f\"--- Finished resizing and saving {source_base_dir.split(os.sep)[-1].capitalize()} set. Total Resized: {total_resized_count}, Total Skipped: {total_skipped_count} ---\")\n    print(f\"Resized data saved to: {destination_base_dir}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:59:01.736812Z","iopub.execute_input":"2025-07-03T12:59:01.737152Z","iopub.status.idle":"2025-07-03T12:59:01.747143Z","shell.execute_reply.started":"2025-07-03T12:59:01.737126Z","shell.execute_reply":"2025-07-03T12:59:01.746353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_SIZE = (224, 224)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:00:14.461155Z","iopub.execute_input":"2025-07-03T13:00:14.461431Z","iopub.status.idle":"2025-07-03T13:00:14.464807Z","shell.execute_reply.started":"2025-07-03T13:00:14.461412Z","shell.execute_reply":"2025-07-03T13:00:14.464143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_train_images, resized_train_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:00:15.845641Z","iopub.execute_input":"2025-07-03T13:00:15.845917Z","iopub.status.idle":"2025-07-03T13:01:04.328738Z","shell.execute_reply.started":"2025-07-03T13:00:15.845897Z","shell.execute_reply":"2025-07-03T13:01:04.328055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_valid_images, resized_valid_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:01:04.329744Z","iopub.execute_input":"2025-07-03T13:01:04.329966Z","iopub.status.idle":"2025-07-03T13:01:21.357665Z","shell.execute_reply.started":"2025-07-03T13:01:04.329949Z","shell.execute_reply":"2025-07-03T13:01:21.356896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_test_images, resized_test_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:01:21.358749Z","iopub.execute_input":"2025-07-03T13:01:21.358961Z","iopub.status.idle":"2025-07-03T13:01:30.260284Z","shell.execute_reply.started":"2025-07-03T13:01:21.358946Z","shell.execute_reply":"2025-07-03T13:01:30.259512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Define model input image size and batch size\nIMAGE_SIZE = (224, 224) # ResNet50 expects 224x224 input\nBATCH_SIZE = 64 # A common and good starting batch size\n\n# Number of classes based on your EMOTION_LABELS dictionary\nNUM_CLASSES = len(emotion_labels) # EMOTION_LABELS should be globally available from Cell 5\n\nprint(f\"Preparing datasets from resized images with size {IMAGE_SIZE} and batch size {BATCH_SIZE}...\")\nprint(f\"Number of emotion classes: {NUM_CLASSES}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:43.031310Z","iopub.execute_input":"2025-07-03T15:39:43.031996Z","iopub.status.idle":"2025-07-03T15:39:43.036276Z","shell.execute_reply.started":"2025-07-03T15:39:43.031957Z","shell.execute_reply":"2025-07-03T15:39:43.035670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data augmentation layers\n# These transformations will be applied randomly to training images on each epoch.\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"), # Randomly flip images horizontally\n    tf.keras.layers.RandomRotation(0.1), # Randomly rotate images by +/- 10% (36 degrees)\n    tf.keras.layers.RandomZoom(0.1), # Randomly zoom in/out by +/- 10%\n    # tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1), # Optional: Randomly shift images\n    # tf.keras.layers.RandomContrast(0.1), # Optional: Randomly adjust contrast\n    # tf.keras.layers.RandomBrightness(0.1), # Optional: Randomly adjust brightness\n], name=\"data_augmentation_layer\") # Name the sequential layer for clarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:43.766832Z","iopub.execute_input":"2025-07-03T15:39:43.767499Z","iopub.status.idle":"2025-07-03T15:39:43.780226Z","shell.execute_reply.started":"2025-07-03T15:39:43.767478Z","shell.execute_reply":"2025-07-03T15:39:43.779752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to apply the ResNet50-specific preprocessing\n# This function handles the normalization (scaling pixels to [-1, 1] based on ImageNet stats)\ndef apply_resnet_preprocessing(image, label):\n    image = resnet_preprocess_input(image)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:44.355859Z","iopub.execute_input":"2025-07-03T15:39:44.356624Z","iopub.status.idle":"2025-07-03T15:39:44.360159Z","shell.execute_reply.started":"2025-07-03T15:39:44.356594Z","shell.execute_reply":"2025-07-03T15:39:44.359591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load the datasets using tf.keras.utils.image_dataset_from_directory\n#    Images are already 224x224 from the resizing step, so image_size here primarily ensures loading consistency.\n#    Labels are inferred from the emotion subfolder names (e.g., 'anger', 'contempt', etc.).\n\n# Training Dataset\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_train_images, # Using your already resized training images\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=True, # Shuffle training data\n    seed=42 # Set seed for reproducibility\n)\n\n# Validation Dataset\nvalid_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_valid_images,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=False, # No need to shuffle validation data\n    seed=42\n)\n\n# Test Dataset\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_test_images,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=False, # No need to shuffle test data\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:44.651200Z","iopub.execute_input":"2025-07-03T15:39:44.651392Z","iopub.status.idle":"2025-07-03T15:39:46.420713Z","shell.execute_reply.started":"2025-07-03T15:39:44.651378Z","shell.execute_reply":"2025-07-03T15:39:46.420117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the class names inferred by Keras (they should match your emotion folder names alphabetically)\nkeras_inferred_class_names = train_ds.class_names\nprint(f\"\\nKeras inferred class names (alphabetical order from folders): {keras_inferred_class_names}\")\n\n# 2. Apply data augmentation to the training dataset ONLY\n#    Then apply ResNet50 preprocessing and optimize dataset loading.\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE) # Apply augmentation\ntrain_ds = train_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Apply ResNet50 preprocessing to validation and test datasets (NO augmentation here)\nvalid_ds = valid_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_ds = test_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"Datasets prepared, augmented (training only), preprocessed, and optimized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:46.422279Z","iopub.execute_input":"2025-07-03T15:39:46.422551Z","iopub.status.idle":"2025-07-03T15:39:46.766576Z","shell.execute_reply.started":"2025-07-03T15:39:46.422533Z","shell.execute_reply":"2025-07-03T15:39:46.766034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Defining ResNet50 model with Data Augmentation and Dropout layers...\")\n\n# Load the ResNet50 base model pre-trained on ImageNet.\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n\n# Initially freeze the entire base model for the first phase of training (feature extraction)\nbase_model.trainable = False\n\n# Create the new model:\n# 1. Input layer (receives original images)\n# 2. Data augmentation layer (applies transformations ONLY during training)\n# 3. Base model (ResNet50 convolutional layers)\n# 4. Custom classification head (GlobalAveragePooling, Dropout, Dense)\n\ninputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\nx = data_augmentation(inputs) # Apply data augmentation\nx = base_model(x, training=False) # Pass through the frozen base model (training=False to ensure BatchNorm layers stay frozen)\nx = GlobalAveragePooling2D()(x) # Reduce spatial dimensions\nx = Dropout(0.5)(x) # Add a Dropout layer (e.g., 50% dropout rate) to prevent overfitting\noutputs = Dense(NUM_CLASSES, activation='softmax')(x) # Output layer for 8 emotion classes\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile the model for the first phase of training (feature extraction)\nmodel.compile(optimizer=Adam(learning_rate=0.001), # Standard learning rate for initial phase\n              loss=SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nprint(\"ResNet50 model (with augmentation and dropout) defined and compiled for initial feature extraction.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:46.767262Z","iopub.execute_input":"2025-07-03T15:39:46.767505Z","iopub.status.idle":"2025-07-03T15:39:47.834304Z","shell.execute_reply.started":"2025-07-03T15:39:46.767467Z","shell.execute_reply":"2025-07-03T15:39:47.833600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Displaying model architecture summary (initial phase - frozen base)...\")\nmodel.summary()\nprint(\"\\nModel architecture summary displayed. Note the 'data_augmentation_layer' and 'Dropout' layer.\")\nprint(\"Also, ResNet50 layers are mostly 'Non-trainable params' in this phase.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:47.836072Z","iopub.execute_input":"2025-07-03T15:39:47.836274Z","iopub.status.idle":"2025-07-03T15:39:47.855457Z","shell.execute_reply.started":"2025-07-03T15:39:47.836259Z","shell.execute_reply":"2025-07-03T15:39:47.854880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'model' is defined and compiled from Cell 23\n\nINITIAL_EPOCHS = 10 # You can adjust this based on how quickly validation loss plateaus.\n\nprint(f\"Starting initial training phase (feature extraction) for {INITIAL_EPOCHS} epochs...\")\n\ncallbacks_initial = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    )\n]\n\nhistory_initial = model.fit(\n    train_ds,\n    epochs=INITIAL_EPOCHS,\n    validation_data=valid_ds,\n    callbacks=callbacks_initial\n)\n\nprint(\"\\nInitial model training (feature extraction) complete.\")\nprint(f\"Training stopped after {len(history_initial.history['loss'])} epochs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:39:47.856285Z","iopub.execute_input":"2025-07-03T15:39:47.856531Z","iopub.status.idle":"2025-07-03T15:51:32.182969Z","shell.execute_reply.started":"2025-07-03T15:39:47.856509Z","shell.execute_reply":"2025-07-03T15:51:32.182234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\nprint(\"Preparing model for fine-tuning phase...\")\n\n# Unfreeze from a specific layer/block\n# ResNet50 has 5 main blocks (conv2_block, conv3_block, conv4_block, conv5_block).\n# It's common to unfreeze conv4_block and conv5_block, or just conv5_block.\n# Let's try unfreezing from 'conv5_block1_1_conv' onwards (which is typically the start of the last major block).\n# Inspect model.summary() to see layer names.\n\n# Freeze all layers in the base_model first\nbase_model.trainable = True # First set trainable for entire base model\n\n# Then, iterate and freeze layers you want to keep frozen\n# The goal is to freeze early layers (generic features) and unfreeze later layers (more specific features)\n# A common practice for ResNet50 is to unfreeze from `conv5_block1_0_conv` or similar later blocks.\n# Let's freeze all layers up to and including 'conv4_block6_out'\nfor layer in base_model.layers:\n    if 'conv5_block' not in layer.name: # Example: Freeze everything before conv5_block\n        layer.trainable = False\n    else:\n        layer.trainable = True\n        # print(f\"Unfrozen layer: {layer.name}\") # Optional: to see which layers are unfrozen\n\n# Verify the number of trainable layers (optional)\n# trainable_count = sum(1 for layer in model.trainable_weights)\n# print(f\"Number of trainable weights after partial unfreezing: {trainable_count}\")\n\n\n# Recompile the model with a much lower learning rate for fine-tuning.\nmodel.compile(optimizer=Adam(learning_rate=0.00001), # Learning rate significantly reduced (e.g., 1e-5)\n              loss=SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nprint(\"Base model partially unfrozen and model recompiled for fine-tuning with a lower learning rate.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:51:32.183840Z","iopub.execute_input":"2025-07-03T15:51:32.184161Z","iopub.status.idle":"2025-07-03T15:51:32.224507Z","shell.execute_reply.started":"2025-07-03T15:51:32.184141Z","shell.execute_reply":"2025-07-03T15:51:32.223901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Displaying model architecture summary (fine-tuning phase - partially unfrozen base)...\")\n# Note the trainable parameters should be higher than initial phase, but lower than full unfreeze.\nmodel.summary()\nprint(\"\\nModel architecture summary displayed. Observe 'Trainable params' for partial unfreezing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:51:32.226671Z","iopub.execute_input":"2025-07-03T15:51:32.227114Z","iopub.status.idle":"2025-07-03T15:51:32.245877Z","shell.execute_reply.started":"2025-07-03T15:51:32.227098Z","shell.execute_reply":"2025-07-03T15:51:32.245353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'history_initial' object from previous phase is available.\n\nFINE_TUNE_EPOCHS = 30 # More epochs for fine-tuning, you can increase this further (e.g., 20-30)\nTOTAL_EPOCHS_CUMULATIVE = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n\nprint(f\"Starting fine-tuning phase for {FINE_TUNE_EPOCHS} additional epochs (Total cumulative epochs: {TOTAL_EPOCHS_CUMULATIVE})...\")\n\ncallbacks_fine_tune = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5, # Increased patience for fine-tuning\n        restore_best_weights=True\n    )\n]\n\nhistory_fine_tune = model.fit(\n    train_ds,\n    epochs=TOTAL_EPOCHS_CUMULATIVE,\n    initial_epoch=history_initial.epoch[-1] + 1, # Continue from where initial training left off\n    validation_data=valid_ds,\n    callbacks=callbacks_fine_tune\n)\n\nprint(\"\\nModel fine-tuning complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T15:51:32.246763Z","iopub.execute_input":"2025-07-03T15:51:32.247032Z","iopub.status.idle":"2025-07-03T16:35:30.301401Z","shell.execute_reply.started":"2025-07-03T15:51:32.247010Z","shell.execute_reply":"2025-07-03T16:35:30.300670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt # Ensure matplotlib.pyplot is imported\n\nprint(\"Plotting cumulative training and validation accuracy and loss...\")\n\n# Get training history data from the initial phase\nacc = history_initial.history['accuracy']\nval_acc = history_initial.history['val_accuracy']\nloss = history_initial.history['loss']\nval_loss = history_initial.history['val_loss']\n\n# Append training history data from the fine-tuning phase\nacc.extend(history_fine_tune.history['accuracy'])\nval_acc.extend(history_fine_tune.history['val_accuracy'])\nloss.extend(history_fine_tune.history['loss'])\nval_loss.extend(history_fine_tune.history['val_loss'])\n\nepochs_range = range(len(acc)) # Create a range for the x-axis, covering all combined epochs\n\nplt.figure(figsize=(12, 5))\n\n# Subplot 1: Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.axvline(x=len(history_initial.history['accuracy']) - 1, color='r', linestyle='--', label='Start Fine-tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy (Cumulative)', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.grid(True)\n\n# Subplot 2: Loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.axvline(x=len(history_initial.history['loss']) - 1, color='r', linestyle='--', label='Start Fine-tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss (Cumulative)', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nCumulative Accuracy and Loss plots displayed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:35:30.302409Z","iopub.execute_input":"2025-07-03T16:35:30.302745Z","iopub.status.idle":"2025-07-03T16:35:30.718205Z","shell.execute_reply.started":"2025-07-03T16:35:30.302717Z","shell.execute_reply":"2025-07-03T16:35:30.717406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the fine-tuned model on the test set...\")\n\nloss, accuracy = model.evaluate(test_ds)\n\nprint(f\"\\nTest Loss (after fine-tuning): {loss:.4f}\")\nprint(f\"Test Accuracy (after fine-tuning): {accuracy:.4f}\")\n\nprint(\"\\nModel testing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:35:30.719175Z","iopub.execute_input":"2025-07-03T16:35:30.719442Z","iopub.status.idle":"2025-07-03T16:35:42.195570Z","shell.execute_reply.started":"2025-07-03T16:35:30.719419Z","shell.execute_reply":"2025-07-03T16:35:42.195019Z"}},"outputs":[],"execution_count":null}]}