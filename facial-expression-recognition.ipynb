{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7595888,"sourceType":"datasetVersion","datasetId":4420036}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial Expression Recognition","metadata":{}},{"cell_type":"markdown","source":"## Import Essential Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport shutil\nimport math\nfrom PIL import Image\nimport cv2\nimport tensorflow as tf\nimport random\nimport pandas as pd\nimport seaborn as sns\n\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Seed","metadata":{}},{"cell_type":"code","source":"# Set all random seeds (Python, NumPy, and TensorFlow)\ntf.keras.utils.set_random_seed(42)\n\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config Tensorflow to use GPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n# List all physical devices (CPUs and GPUs) that TensorFlow can see\nphysical_devices = tf.config.list_physical_devices()\nprint(f\"Physical devices detected: {physical_devices}\")\n\n# Specifically list GPUs\ngpu_devices = tf.config.list_physical_devices('GPU')\nif gpu_devices:\n    print(f\"\\nNumber of GPUs available: {len(gpu_devices)}\")\n    for i, gpu in enumerate(gpu_devices):\n        print(f\"  GPU {i}: {gpu}\")\n    print(\"\\nTensorFlow will automatically use the GPU if available.\")\nelse:\n    print(\"\\nNo GPU devices found. TensorFlow will run on CPU.\")\n\n# You can also check if a random tensor is placed on GPU by default\n# This should show GPU if one is available and being used\ntest_tensor = tf.constant([1.0, 2.0, 3.0])\nprint(f\"\\nDefault device for a tensor: {test_tensor.device}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Global Variables","metadata":{}},{"cell_type":"code","source":"# List of emotions\nemotions = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of emotion lables\nemotion_labels = {\n    0: 'anger',\n    1: 'contempt',\n    2: 'disgust',\n    3: 'fear',\n    4: 'happy',\n    5: 'neutral',\n    6: 'sad',\n    7: 'surprise'\n}","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Global URLs","metadata":{}},{"cell_type":"code","source":"# Input related URLs\ninput_base_url = '/kaggle/input/'\n\ntrain_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/train/images')\nvalid_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/valid/images')\ntest_images_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/test/images')\n\ntrain_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/train/labels')\nvalid_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/valid/labels')\ntest_labels_url = os.path.join(input_base_url, 'affectnet-yolo-format/YOLO_format/test/labels')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output related URLs\noutput_base_url = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Organized Images paths\norganized_base_dir = os.path.join(output_base_url, 'organized_images')\n\norganized_train_images = os.path.join(organized_base_dir, 'train')\norganized_valid_images = os.path.join(organized_base_dir, 'valid')\norganized_test_images = os.path.join(organized_base_dir, 'test')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resized Images paths\nresized_base_dir = os.path.join(output_base_url, 'resized_images')\n\nresized_train_images = os.path.join(resized_base_dir, 'train')\nresized_valid_images = os.path.join(resized_base_dir, 'valid')\nresized_test_images = os.path.join(resized_base_dir, 'test')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Balanced Images paths\nbalanced_base_dir = os.path.join(output_base_url, 'balanced_images')\n\nbalanced_train_images = os.path.join(balanced_base_dir, 'train')\nbalanced_valid_images = os.path.join(balanced_base_dir, 'valid')\nbalanced_test_images = os.path.join(balanced_base_dir, 'test')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"# Create directories\nos.makedirs(organized_base_dir, exist_ok=True)\nos.makedirs(resized_base_dir, exist_ok=True)\nos.makedirs(balanced_base_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split Images into Emotion Folders","metadata":{}},{"cell_type":"code","source":"def reorganize_dataset(source_images_dir, source_labels_dir, destination_base_dir, emotion_map):\n    \"\"\"\n    Reorganizes image files into emotion-specific subfolders based on YOLO-format label files.\n\n    Args:\n        source_images_dir (str): Path to the directory containing image files (e.g., train/images).\n        source_labels_dir (str): Path to the directory containing label .txt files (e.g., train/labels).\n        destination_base_dir (str): Path to the root directory where reorganized data will be saved.\n                                    (e.g., /kaggle/working/processed_train)\n        emotion_map (dict): A dictionary mapping integer class IDs to emotion names (e.g., {0: 'anger'}).\n    \"\"\"\n    print(f\"--- Reorganizing: {source_images_dir.split('/')[-2]} set ---\")\n\n    # Create destination directories for each emotion\n    for emotion_id, emotion_name in emotion_map.items():\n        # Using the emotion name (e.g., 'anger') as the subfolder name\n        class_folder = os.path.join(destination_base_dir, emotion_name)\n        os.makedirs(class_folder, exist_ok=True) # exist_ok=True prevents error if folder already exists\n        print(f\"  Created directory: {class_folder}\")\n\n    # Iterate through each image file\n    image_files = [f for f in os.listdir(source_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    print(f\"  Found {len(image_files)} image files in {source_images_dir}\")\n\n    processed_count = 0\n    skipped_count = 0\n\n    for img_filename in image_files:\n        img_path = os.path.join(source_images_dir, img_filename)\n        \n        # Construct the corresponding label file name\n        # Remove the image extension (.png, .jpg) and replace with .txt\n        base_filename = os.path.splitext(img_filename)[0]\n        label_filename = base_filename + '.txt'\n        label_path = os.path.join(source_labels_dir, label_filename)\n\n        if not os.path.exists(label_path):\n            print(f\"    Warning: Label file not found for {img_filename} at {label_path}. Skipping.\")\n            skipped_count += 1\n            continue\n\n        try:\n            with open(label_path, 'r') as f:\n                # Read the first line (assuming one object/emotion per image)\n                label_line = f.readline().strip()\n                # Extract the class_id (first number)\n                class_id = int(label_line.split(' ')[0])\n\n            emotion_name = emotion_map.get(class_id)\n            if emotion_name is None:\n                print(f\"    Warning: Unknown class_id {class_id} for {img_filename}. Skipping.\")\n                skipped_count += 1\n                continue\n\n            destination_folder = os.path.join(destination_base_dir, emotion_name)\n            destination_path = os.path.join(destination_folder, img_filename)\n\n            # Copy the image file\n            shutil.copy(img_path, destination_path)\n            processed_count += 1\n\n            if processed_count % 1000 == 0:\n                print(f\"    Processed {processed_count} images for {source_images_dir.split('/')[-2]} set...\")\n\n        except Exception as e:\n            print(f\"    Error processing {img_filename} or {label_filename}: {e}. Skipping.\")\n            skipped_count += 1\n            continue\n\n    print(f\"--- Finished reorganizing {source_images_dir.split('/')[-2]} set. Processed: {processed_count}, Skipped: {skipped_count} ---\")\n    print(f\"Reorganized data is in: {destination_base_dir}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reorganize the training data\nreorganize_dataset(train_images_url, train_labels_url, organized_train_images, emotion_labels)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(organized_valid_images, exist_ok=True)\n\n# Reorganize the validation data\nreorganize_dataset(valid_images_url, valid_labels_url, organized_valid_images, emotion_labels)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(organized_test_images, exist_ok=True)\n\n# Reorganize the validation data\nreorganize_dataset(test_images_url, test_labels_url, organized_test_images, emotion_labels)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check if All of Images are RGBs","metadata":{}},{"cell_type":"code","source":"def check_all_image_color_modes_strict_rgb(base_directory):\n    \"\"\"\n    Checks the color mode of ALL images within a given base directory and its subfolders,\n    specifically verifying if they are ONLY 'RGB' (3-channel color).\n\n    Args:\n        base_directory (str): The root directory of the dataset split to check.\n\n    Returns:\n        tuple: A tuple containing:\n               - all_unique_modes (set): A set of all unique color modes found across all images.\n               - non_rgb_paths (list): A list of paths to ALL files that are NOT 'RGB'.\n    \"\"\"\n    if not os.path.exists(base_directory):\n        print(f\"Error: Directory not found: {base_directory}\")\n        return set(), []\n\n    all_unique_modes = set()\n    non_rgb_paths = [] # To store paths of ALL images found not to be strictly 'RGB'\n    \n    print(f\"--- Scanning directory: {base_directory} for image color modes (checking ALL images for 'RGB' only) ---\")\n\n    # Get the list of emotion subfolders (e.g., 'anger', 'contempt')\n    emotion_folders = [d for d in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, d))]\n    \n    if not emotion_folders:\n        print(f\"  No emotion subfolders found in {base_directory}. Cannot perform color mode check.\")\n        return set(), []\n\n    processed_count = 0\n    # Iterate through each emotion folder\n    for emotion_folder in emotion_folders:\n        emotion_path = os.path.join(base_directory, emotion_folder)\n        # List all common image files in the current emotion folder\n        image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))]\n        \n        # Iterate through each image file\n        for img_filename in image_files:\n            img_path = os.path.join(emotion_path, img_filename)\n            try:\n                with Image.open(img_path) as img:\n                    all_unique_modes.add(img.mode) # Add the image's mode to the set of unique modes\n                    \n                    # If the mode is NOT strictly 'RGB', add its path to our list\n                    if img.mode != 'RGB':\n                        non_rgb_paths.append(img_path)\n            except Exception as e:\n                print(f\"  Warning: Could not open {img_path} to check mode: {e}. Skipping this file.\")\n            \n            processed_count += 1\n            # Print progress every 1000 images\n            if processed_count % 1000 == 0:\n                print(f\"  Processed {processed_count} images in {base_directory}...\")\n\n    print(f\"Scan complete for {base_directory}. Total images processed: {processed_count}.\")\n    \n    if all_unique_modes:\n        print(f\"Unique color modes found across all images: {sorted(list(all_unique_modes))}\")\n    else:\n        print(\"No image files found for mode check.\")\n        \n    if non_rgb_paths:\n        print(f\"WARNING: Detected {len(non_rgb_paths)} images that are NOT 'RGB'.\")\n        print(\"  These might include 'L' (grayscale), 'P' (palettized), 'RGBA' (color with alpha), etc.\")\n        print(\"  Sample paths of non-'RGB' images (up to 10 for brevity):\")\n        for p in non_rgb_paths[:10]: # Print only a small sample to avoid huge output\n            print(f\"    {p}\")\n    else:\n        print(\"All images found are exclusively 'RGB' (3-channel color).\")\n        \n    return all_unique_modes, non_rgb_paths","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming organized_train_images, organized_valid_images, organized_test_images are defined from your global variables.\n\nprint(\"--- Starting Full STRICT 'RGB' Color Mode Check on ORGANIZED Images ---\")\n\nprint(\"\\n--- Checking Organized Training Images ---\")\ntrain_modes_strict, train_non_rgb_paths_strict = check_all_image_color_modes_strict_rgb(organized_train_images)\n\nprint(\"\\n--- Checking Organized Validation Images ---\")\nvalid_modes_strict, valid_non_rgb_paths_strict = check_all_image_color_modes_strict_rgb(organized_valid_images)\n\nprint(\"\\n--- Checking Organized Test Images ---\")\ntest_modes_strict, test_non_rgb_paths_strict = check_all_image_color_modes_strict_rgb(organized_test_images)\n\nprint(\"\\n--- Full Strict 'RGB' Color Mode Check on Organized Directories Complete ---\")\n\n# The variables train_modes_strict, valid_modes_strict, test_modes_strict will contain sets like {'RGB', 'L', 'P', 'RGBA'}\n# The lists train_non_rgb_paths_strict, etc., will contain paths to any images that were NOT strictly 'RGB'.","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Verify Organized Images Directories","metadata":{}},{"cell_type":"code","source":"# Define a function to count images in the reorganized structure\ndef count_images_in_organized_dirs(base_dir, emotion_list):\n    \"\"\"\n    Counts images in emotion subfolders within a given base directory.\n\n    Args:\n        base_dir (str): The root directory of the organized dataset split (e.g., '/kaggle/working/organized_images/train').\n        emotion_list (list): A list of emotion names (which are also the subfolder names).\n\n    Returns:\n        dict: A dictionary with emotion names as keys and image counts as values.\n    \"\"\"\n    counts = {}\n    total_images = 0\n    print(f\"--- Checking: {base_dir.split('/')[-1].capitalize()} Set ---\")\n\n    if not os.path.exists(base_dir):\n        print(f\"  Warning: Base directory not found: {base_dir}. Skipping.\")\n        return counts, 0\n\n    for emotion in emotion_list:\n        emotion_path = os.path.join(base_dir, emotion)\n        if os.path.exists(emotion_path) and os.path.isdir(emotion_path):\n            # Count only files (not subdirectories)\n            num_images = len([f for f in os.listdir(emotion_path) if os.path.isfile(os.path.join(emotion_path, f))])\n            counts[emotion] = num_images\n            total_images += num_images\n            print(f\"  {emotion.capitalize()}: {num_images} images\")\n        else:\n            print(f\"  Warning: Emotion directory not found for {emotion} in {base_dir}.\")\n            counts[emotion] = 0\n\n    print(f\"  Total images in {base_dir.split('/')[-1].capitalize()} Set: {total_images}\\n\")\n    return counts, total_images","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Training Set\ntrain_counts, total_train = count_images_in_organized_dirs(organized_train_images, emotions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Validation Set\nvalid_counts, total_valid = count_images_in_organized_dirs(organized_valid_images, emotions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get counts for Test Set\ntest_counts, total_test = count_images_in_organized_dirs(organized_test_images, emotions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n--- Overall Summary of Reorganized Dataset ---\")\nprint(f\"Total images across all sets: {total_train + total_valid + total_test}\")\nprint(f\"Train Set Total: {total_train}\")\nprint(f\"Valid Set Total: {total_valid}\")\nprint(f\"Test Set Total: {total_test}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store counts in global variables for future use (e.g., plotting in the next step)\nglobals()['train_counts'] = train_counts\nglobals()['valid_counts'] = valid_counts\nglobals()['test_counts'] = test_counts","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Class Distribution of Train, Valid and Test","metadata":{}},{"cell_type":"code","source":"print(\"Preparing DataFrames for class distribution plots...\")\n\n# Create DataFrames for each split\ntrain_df = pd.DataFrame(list(train_counts.items()), columns=['Emotion', 'Count'])\ntrain_df['Set'] = 'Train' # Add a 'Set' column (useful if we wanted a combined plot later)\n\nvalid_df = pd.DataFrame(list(valid_counts.items()), columns=['Emotion', 'Count'])\nvalid_df['Set'] = 'Validation'\n\ntest_df = pd.DataFrame(list(test_counts.items()), columns=['Emotion', 'Count'])\ntest_df['Set'] = 'Test'\n\nprint(\"DataFrames (train_df, valid_df, test_df) created.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_class_distribution(df, title):\n    \"\"\"\n    Plots the class distribution for a given dataset split.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing 'Emotion' and 'Count' columns.\n        title (str): The title for the plot.\n    \"\"\"\n    plt.figure(figsize=(10, 6)) # Adjust figure size as needed\n    sns.barplot(x='Emotion', y='Count', data=df, palette='viridis')\n    plt.title(title, fontsize=16)\n    plt.xlabel('Emotion', fontsize=12)\n    plt.ylabel('Number of Images', fontsize=12)\n    plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate labels for readability\n    plt.yticks(fontsize=10)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout() # Adjust layout to prevent elements from overlapping\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot train directory class distribution\nplot_class_distribution(train_df, 'Training Set Class Distribution')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot valid directory class distribution\nplot_class_distribution(valid_df, 'Validation Set Class Distribution')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot test directory class distribution\nplot_class_distribution(test_df, 'Test Set Class Distribution')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Images","metadata":{}},{"cell_type":"code","source":"def plot_emotion_samples_organized(dataset_split_base_dir: str, emotion_name: str, num_images_to_plot: int, images_per_row: int = 10):\n    \"\"\"\n    Plots a specified number of sample images for a given emotion from an organized dataset split,\n    preserving original image colors.\n\n    Args:\n        dataset_split_base_dir (str): The root directory of the organized dataset split (e.g., organized_train_images).\n        emotion_name (str): The name of the emotion (e.g., 'happy', 'sad'). This should match the subfolder name.\n        num_images_to_plot (int): The maximum number of images to plot for this emotion.\n        images_per_row (int, optional): How many images to display horizontally in each row.\n                                        Defaults to 10.\n    \"\"\"\n    emotion_path = os.path.join(dataset_split_base_dir, emotion_name)\n\n    if not os.path.exists(emotion_path):\n        print(f\"Error: Directory not found for '{emotion_name}' at '{emotion_path}'. Skipping plot.\")\n        return\n\n    image_files = [f for f in os.listdir(emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n\n    if not image_files:\n        print(f\"No images found in '{emotion_path}' for emotion '{emotion_name}'. Skipping plot.\")\n        return\n\n    images_to_display = random.sample(image_files, min(len(image_files), num_images_to_plot))\n\n    num_images_actual = len(images_to_display)\n    if num_images_actual == 0:\n        print(f\"No images selected for display for '{emotion_name}'. Skipping plot.\")\n        return\n\n    num_rows = math.ceil(num_images_actual / images_per_row)\n\n    fig_width = images_per_row * 1.8 # Adjusted for slightly larger color images\n    fig_height = num_rows * 2.2 # Slightly more height per row\n    \n    plt.figure(figsize=(fig_width, fig_height))\n    \n    split_name = dataset_split_base_dir.split(os.sep)[-1].capitalize() \n    plt.suptitle(f\"Sample Images: {emotion_name.capitalize()} ({split_name} Set)\", fontsize=18, y=1.02)\n\n    for i, img_filename in enumerate(images_to_display):\n        img_path = os.path.join(emotion_path, img_filename)\n\n        try:\n            img = Image.open(img_path) \n            \n            ax = plt.subplot(num_rows, images_per_row, i + 1)\n            ax.imshow(img) \n            ax.set_title(f\"#{i+1}\", fontsize=8)\n            ax.axis('off')\n\n        except Exception as e:\n            print(f\"  Warning: Could not load image '{img_filename}' from '{emotion_path}': {e}. Skipping.\")\n            ax = plt.subplot(num_rows, images_per_row, i + 1)\n            ax.text(0.5, 0.5, 'Error', ha='center', va='center', fontsize=12, color='red')\n            ax.axis('off')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n    plt.show()\n    print(f\"Finished plotting {num_images_actual} images for '{emotion_name}' in the {split_name} set.\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_IMAGES = 20","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Training Set Images ---\")\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_train_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Validation Set Images ---\")\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_valid_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"--- Visualizing Test Set Images ---\")\nfor emotion in emotions:\n    plot_emotion_samples_organized(organized_test_images, emotion, NUM_IMAGES)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Resize Images to 224x224","metadata":{}},{"cell_type":"code","source":"def resize_and_save_dataset(source_base_dir, destination_base_dir, target_size=(224, 224)):\n    \"\"\"\n    Resizes all images in emotion subfolders from a source directory and saves them\n    to a new destination directory, maintaining the folder structure and preserving color.\n\n    Args:\n        source_base_dir (str): Path to the root directory of the already organized dataset split\n                               (e.g., '/kaggle/working/organized_images/train').\n        destination_base_dir (str): Path to the root directory where resized images will be saved.\n                                    (e.g., '/kaggle/working/resized_images/train')\n        target_size (tuple): A tuple (width, height) for the new image size.\n    \"\"\"\n    print(f\"--- Resizing and saving images for: {source_base_dir.split(os.sep)[-1].capitalize()} set to {target_size} ---\")\n\n    if not os.path.exists(source_base_dir):\n        print(f\"  Error: Source directory not found: {source_base_dir}. Skipping.\")\n        return\n\n    os.makedirs(destination_base_dir, exist_ok=True)\n\n    emotions_in_source = [d for d in os.listdir(source_base_dir) if os.path.isdir(os.path.join(source_base_dir, d))]\n    \n    if not emotions_in_source:\n        print(f\"  No emotion subfolders found in {source_base_dir}. Skipping.\")\n        return\n\n    total_resized_count = 0\n    total_skipped_count = 0\n\n    for emotion_folder in emotions_in_source:\n        source_emotion_path = os.path.join(source_base_dir, emotion_folder)\n        destination_emotion_path = os.path.join(destination_base_dir, emotion_folder)\n\n        os.makedirs(destination_emotion_path, exist_ok=True)\n\n        image_files = [f for f in os.listdir(source_emotion_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n\n        resized_count_per_emotion = 0\n        skipped_count_per_emotion = 0\n\n        for img_filename in image_files:\n            source_img_path = os.path.join(source_emotion_path, img_filename)\n            destination_img_path = os.path.join(destination_emotion_path, img_filename)\n\n            try:\n                # Open image in its original mode (e.g., RGB)\n                img = Image.open(source_img_path)\n                # Convert to RGB just in case it's a palette or RGBA image, to ensure 3 channels\n                if img.mode != 'RGB':\n                    img = img.convert('RGB')\n                \n                # Resize image using LANCZOS for high-quality upscaling/downsampling\n                resized_img = img.resize(target_size, Image.LANCZOS)\n                \n                resized_img.save(destination_img_path)\n                resized_count_per_emotion += 1\n                total_resized_count += 1\n                \n                if resized_count_per_emotion % 1000 == 0:\n                    print(f\"      Processed {resized_count_per_emotion} images in {emotion_folder} for {source_base_dir.split(os.sep)[-1].capitalize()} set...\")\n\n            except Exception as e:\n                print(f\"    Error resizing {img_filename} from {source_emotion_path}: {e}. Skipping.\")\n                skipped_count_per_emotion += 1\n                total_skipped_count += 1\n                continue\n        \n    print(f\"--- Finished resizing and saving {source_base_dir.split(os.sep)[-1].capitalize()} set. Total Resized: {total_resized_count}, Total Skipped: {total_skipped_count} ---\")\n    print(f\"Resized data saved to: {destination_base_dir}\\n\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_SIZE = (224, 224)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_train_images, resized_train_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_valid_images, resized_valid_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_and_save_dataset(organized_test_images, resized_test_images, target_size=IMAGE_SIZE)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"print(\"Defining the Keras Data Augmentation layer (tf.keras.Sequential model)...\")\n\ndata_augmentation_pipeline = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.1),\n    tf.keras.layers.RandomZoom(0.1),\n    # Add more robust augmentation layers here as needed:\n    # tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n    # tf.keras.layers.RandomContrast(0.1),\n    # tf.keras.layers.RandomBrightness(0.1),\n], name=\"data_augmentation_pipeline\")\n\nprint(\"Data augmentation Sequential layer 'data_augmentation_pipeline' defined.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_and_save_augmented_images(\n    source_emotion_dir,\n    dest_emotion_dir,\n    current_count,\n    target_count_per_class,\n    augmentation_model, # The tf.keras.Sequential model for augmentation\n    image_size=(224, 224)\n):\n    \"\"\"\n    Generates and saves augmented images for a specific emotion class to reach a target count.\n    First, it copies existing images, then generates augmented ones.\n\n    Args:\n        source_emotion_dir (str): Path to the source directory for the current emotion.\n        dest_emotion_dir (str): Path to the destination directory for the current emotion.\n        current_count (int): Current number of images in the source_emotion_dir.\n        target_count_per_class (int): Desired total number of images per class after balancing.\n        augmentation_model (tf.keras.Sequential): Keras Sequential model with augmentation layers.\n        image_size (tuple): Tuple (height, width) for the images.\n    \"\"\"\n    os.makedirs(dest_emotion_dir, exist_ok=True)\n\n    image_files = [f for f in os.listdir(source_emotion_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n    \n    # 1. Copy all existing images to the new balanced directory\n    print(f\"  Copying {len(image_files)} existing images from {source_emotion_dir} to {dest_emotion_dir}...\")\n    for img_filename in image_files:\n        try:\n            shutil.copy(os.path.join(source_emotion_dir, img_filename), os.path.join(dest_emotion_dir, img_filename))\n        except Exception as e:\n            print(f\"    Warning: Error copying {img_filename}: {e}. Skipping copy.\")\n            \n    # Calculate how many new augmented images are needed\n    images_to_generate = target_count_per_class - current_count\n    \n    if images_to_generate <= 0:\n        print(f\"  Class '{os.path.basename(source_emotion_dir)}' already has {current_count} images (target {target_count_per_class}). No new images to generate.\")\n        return\n\n    print(f\"  Generating {images_to_generate} new augmented images for '{os.path.basename(source_emotion_dir)}'...\")\n\n    loaded_images = []\n    for img_filename in image_files:\n        img_path = os.path.join(source_emotion_dir, img_filename)\n        try:\n            img = Image.open(img_path).convert('RGB')\n            loaded_images.append(np.array(img, dtype=np.float32))\n        except Exception as e:\n            print(f\"    Warning: Could not load {img_path} for augmentation generation: {e}. Skipping.\")\n    \n    if not loaded_images:\n        print(f\"    No images successfully loaded from {source_emotion_dir}. Cannot generate augmented images.\")\n        return\n\n    for i in range(images_to_generate):\n        original_img_np = random.choice(loaded_images)\n        original_img_tensor = tf.expand_dims(original_img_np, 0)\n        \n        augmented_tensor = augmentation_model(original_img_tensor, training=True)\n        augmented_img_np = augmented_tensor[0].numpy().astype(\"uint8\")\n        augmented_img_pil = Image.fromarray(augmented_img_np, 'RGB')\n        \n        original_base_name = os.path.splitext(random.choice(image_files))[0]\n        new_filename = f\"{original_base_name}_aug_{i:04d}_{random.randint(0,9999)}.png\"\n        save_path = os.path.join(dest_emotion_dir, new_filename)\n        \n        try:\n            augmented_img_pil.save(save_path)\n        except Exception as e:\n            print(f\"    Error saving augmented image {new_filename} to {dest_emotion_dir}: {e}. Skipping.\")\n            continue\n\n        if (i + 1) % 100 == 0 or (i + 1) == images_to_generate:\n            print(f\"    Generated {i+1}/{images_to_generate} images for '{os.path.basename(source_emotion_dir)}'...\")\n    \n    print(f\"  Finished generating augmented images for '{os.path.basename(source_emotion_dir)}'. Total images now: {target_count_per_class}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_count_per_class_train = 4000 # Your specified target for each training class\n\nprint(f\"Starting data balancing for training set. Each class will aim for {target_count_per_class_train} images.\")\n\nfor emotion_id, emotion_name in emotion_labels.items():\n    current_count = train_counts.get(emotion_name, 0)\n\n    source_dir_emotion = os.path.join(resized_train_images, emotion_name)\n    dest_dir_emotion = os.path.join(balanced_train_images, emotion_name)\n\n    generate_and_save_augmented_images(\n        source_dir_emotion,\n        dest_dir_emotion,\n        current_count,\n        target_count_per_class_train,\n        data_augmentation_pipeline, # Use the defined augmentation Sequential model\n        image_size=IMAGE_SIZE\n    )\n\nprint(\"\\nTraining data balancing process complete.\")\nprint(\"The 'balanced_train_images' directory now contains the augmented and copied training data.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def copy_split_for_balancing_structure(source_path, dest_path):\n    set_name = source_path.split(os.sep)[-1]\n    print(f\"--- Copying {set_name.capitalize()} Set (no balancing/augmentation) ---\")\n    if os.path.exists(dest_path):\n        print(f\"  Removing existing content in {dest_path} before copying...\")\n        shutil.rmtree(dest_path)\n    os.makedirs(dest_path, exist_ok=True)\n    \n    shutil.copytree(source_path, dest_path, dirs_exist_ok=True)\n    print(f\"  Finished copying {set_name} to {dest_path}.\\n\")\n\ncopy_split_for_balancing_structure(resized_valid_images, balanced_valid_images)\ncopy_split_for_balancing_structure(resized_test_images, balanced_test_images)\n\nprint(\"Validation and Test sets copied to 'balanced_images' structure.\")\nprint(\"Your 'balanced_base_dir' is now ready to use for Keras Datasets.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming train_counts is available from Cell 12\nprint(\"Calculating class weights for balanced training dataset (for potential loss adjustment)...\")\n\n# Get the counts for the newly balanced training set to see the effect\n# This will count the images in the BALANCED_TRAIN_IMAGES_DIR\nbalanced_train_counts, _ = count_images_in_organized_dirs(balanced_train_images, emotions)\n\n# Convert counts to an ordered list based on emotion_labels order (0 to 7)\nbalanced_train_counts_ordered = [balanced_train_counts[emotion_labels[i]] for i in range(len(emotion_labels))]\n\ntotal_samples_balanced_train = sum(balanced_train_counts_ordered)\nmax_samples_balanced_train = float(max(balanced_train_counts_ordered))\n\nclass_weights = {}\nfor i, count in enumerate(balanced_train_counts_ordered):\n    class_weights[i] = max_samples_balanced_train / count\n\nprint(\"\\nCalculated Class Weights (for SparseCategoricalCrossentropy):\")\nfor idx, weight in class_weights.items():\n    print(f\"  {emotion_labels[idx].capitalize()} (ID {idx}): Count={balanced_train_counts_ordered[idx]}, Weight={weight:.2f}\")\n\n# Store class_weights in a global variable for use in model.fit()\n# Using snake_case for consistency\nclass_weights_dict = class_weights\n\nprint(\"\\nClass weights calculated and stored in class_weights_dict global variable.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalize","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Define model input image size and batch size\nIMAGE_SIZE = (224, 224) # ResNet50 expects 224x224 input\nBATCH_SIZE = 64 # A common and good starting batch size\n\n# Number of classes based on your EMOTION_LABELS dictionary\nNUM_CLASSES = len(emotion_labels) # EMOTION_LABELS should be globally available from Cell 5\n\nprint(f\"Preparing datasets from resized images with size {IMAGE_SIZE} and batch size {BATCH_SIZE}...\")\nprint(f\"Number of emotion classes: {NUM_CLASSES}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the data augmentation layers\n# These transformations will be applied randomly to training images on each epoch.\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"), # Randomly flip images horizontally\n    tf.keras.layers.RandomRotation(0.1), # Randomly rotate images by +/- 10% (36 degrees)\n    tf.keras.layers.RandomZoom(0.1), # Randomly zoom in/out by +/- 10%\n    # tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1), # Optional: Randomly shift images\n    # tf.keras.layers.RandomContrast(0.1), # Optional: Randomly adjust contrast\n    # tf.keras.layers.RandomBrightness(0.1), # Optional: Randomly adjust brightness\n], name=\"data_augmentation_layer\") # Name the sequential layer for clarity","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function to apply the ResNet50-specific preprocessing\n# This function handles the normalization (scaling pixels to [-1, 1] based on ImageNet stats)\ndef apply_resnet_preprocessing(image, label):\n    image = resnet_preprocess_input(image)\n    return image, label","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load the datasets using tf.keras.utils.image_dataset_from_directory\n#    Images are already 224x224 from the resizing step, so image_size here primarily ensures loading consistency.\n#    Labels are inferred from the emotion subfolder names (e.g., 'anger', 'contempt', etc.).\n\n# Training Dataset\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_train_images, # Using your already resized training images\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=True, # Shuffle training data\n    seed=42 # Set seed for reproducibility\n)\n\n# Validation Dataset\nvalid_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_valid_images,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=False, # No need to shuffle validation data\n    seed=42\n)\n\n# Test Dataset\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    resized_test_images,\n    labels='inferred',\n    label_mode='int',\n    image_size=IMAGE_SIZE,\n    interpolation='bilinear',\n    batch_size=BATCH_SIZE,\n    shuffle=False, # No need to shuffle test data\n    seed=42\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the class names inferred by Keras (they should match your emotion folder names alphabetically)\nkeras_inferred_class_names = train_ds.class_names\nprint(f\"\\nKeras inferred class names (alphabetical order from folders): {keras_inferred_class_names}\")\n\n# 2. Apply data augmentation to the training dataset ONLY\n#    Then apply ResNet50 preprocessing and optimize dataset loading.\ntrain_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE) # Apply augmentation\ntrain_ds = train_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Apply ResNet50 preprocessing to validation and test datasets (NO augmentation here)\nvalid_ds = valid_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_ds = test_ds.map(apply_resnet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"Datasets prepared, augmented (training only), preprocessed, and optimized.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Defining ResNet50 model with Data Augmentation and Dropout layers...\")\n\n# Load the ResNet50 base model pre-trained on ImageNet.\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n\n# Initially freeze the entire base model for the first phase of training (feature extraction)\nbase_model.trainable = False\n\n# Create the new model:\n# 1. Input layer (receives original images)\n# 2. Data augmentation layer (applies transformations ONLY during training)\n# 3. Base model (ResNet50 convolutional layers)\n# 4. Custom classification head (GlobalAveragePooling, Dropout, Dense)\n\ninputs = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\nx = data_augmentation(inputs) # Apply data augmentation\nx = base_model(x, training=False) # Pass through the frozen base model (training=False to ensure BatchNorm layers stay frozen)\nx = GlobalAveragePooling2D()(x) # Reduce spatial dimensions\nx = Dropout(0.5)(x) # Add a Dropout layer (e.g., 50% dropout rate) to prevent overfitting\noutputs = Dense(NUM_CLASSES, activation='softmax')(x) # Output layer for 8 emotion classes\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile the model for the first phase of training (feature extraction)\nmodel.compile(optimizer=Adam(learning_rate=0.001), # Standard learning rate for initial phase\n              loss=SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nprint(\"ResNet50 model (with augmentation and dropout) defined and compiled for initial feature extraction.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Displaying model architecture summary (initial phase - frozen base)...\")\nmodel.summary()\nprint(\"\\nModel architecture summary displayed. Note the 'data_augmentation_layer' and 'Dropout' layer.\")\nprint(\"Also, ResNet50 layers are mostly 'Non-trainable params' in this phase.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'model' is defined and compiled from Cell 23\n\nINITIAL_EPOCHS = 10 # You can adjust this based on how quickly validation loss plateaus.\n\nprint(f\"Starting initial training phase (feature extraction) for {INITIAL_EPOCHS} epochs...\")\n\ncallbacks_initial = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    )\n]\n\nhistory_initial = model.fit(\n    train_ds,\n    epochs=INITIAL_EPOCHS,\n    validation_data=valid_ds,\n    callbacks=callbacks_initial\n)\n\nprint(\"\\nInitial model training (feature extraction) complete.\")\nprint(f\"Training stopped after {len(history_initial.history['loss'])} epochs.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\nprint(\"Preparing model for fine-tuning phase...\")\n\n# Unfreeze from a specific layer/block\n# ResNet50 has 5 main blocks (conv2_block, conv3_block, conv4_block, conv5_block).\n# It's common to unfreeze conv4_block and conv5_block, or just conv5_block.\n# Let's try unfreezing from 'conv5_block1_1_conv' onwards (which is typically the start of the last major block).\n# Inspect model.summary() to see layer names.\n\n# Freeze all layers in the base_model first\nbase_model.trainable = True # First set trainable for entire base model\n\n# Then, iterate and freeze layers you want to keep frozen\n# The goal is to freeze early layers (generic features) and unfreeze later layers (more specific features)\n# A common practice for ResNet50 is to unfreeze from `conv5_block1_0_conv` or similar later blocks.\n# Let's freeze all layers up to and including 'conv4_block6_out'\nfor layer in base_model.layers:\n    if 'conv5_block' not in layer.name: # Example: Freeze everything before conv5_block\n        layer.trainable = False\n    else:\n        layer.trainable = True\n        # print(f\"Unfrozen layer: {layer.name}\") # Optional: to see which layers are unfrozen\n\n# Verify the number of trainable layers (optional)\n# trainable_count = sum(1 for layer in model.trainable_weights)\n# print(f\"Number of trainable weights after partial unfreezing: {trainable_count}\")\n\n\n# Recompile the model with a much lower learning rate for fine-tuning.\nmodel.compile(optimizer=Adam(learning_rate=0.00001), # Learning rate significantly reduced (e.g., 1e-5)\n              loss=SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nprint(\"Base model partially unfrozen and model recompiled for fine-tuning with a lower learning rate.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Displaying model architecture summary (fine-tuning phase - partially unfrozen base)...\")\n# Note the trainable parameters should be higher than initial phase, but lower than full unfreeze.\nmodel.summary()\nprint(\"\\nModel architecture summary displayed. Observe 'Trainable params' for partial unfreezing.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'history_initial' object from previous phase is available.\n\nFINE_TUNE_EPOCHS = 30 # More epochs for fine-tuning, you can increase this further (e.g., 20-30)\nTOTAL_EPOCHS_CUMULATIVE = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n\nprint(f\"Starting fine-tuning phase for {FINE_TUNE_EPOCHS} additional epochs (Total cumulative epochs: {TOTAL_EPOCHS_CUMULATIVE})...\")\n\ncallbacks_fine_tune = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5, # Increased patience for fine-tuning\n        restore_best_weights=True\n    )\n]\n\nhistory_fine_tune = model.fit(\n    train_ds,\n    epochs=TOTAL_EPOCHS_CUMULATIVE,\n    initial_epoch=history_initial.epoch[-1] + 1, # Continue from where initial training left off\n    validation_data=valid_ds,\n    callbacks=callbacks_fine_tune\n)\n\nprint(\"\\nModel fine-tuning complete.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt # Ensure matplotlib.pyplot is imported\n\nprint(\"Plotting cumulative training and validation accuracy and loss...\")\n\n# Get training history data from the initial phase\nacc = history_initial.history['accuracy']\nval_acc = history_initial.history['val_accuracy']\nloss = history_initial.history['loss']\nval_loss = history_initial.history['val_loss']\n\n# Append training history data from the fine-tuning phase\nacc.extend(history_fine_tune.history['accuracy'])\nval_acc.extend(history_fine_tune.history['val_accuracy'])\nloss.extend(history_fine_tune.history['loss'])\nval_loss.extend(history_fine_tune.history['val_loss'])\n\nepochs_range = range(len(acc)) # Create a range for the x-axis, covering all combined epochs\n\nplt.figure(figsize=(12, 5))\n\n# Subplot 1: Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.axvline(x=len(history_initial.history['accuracy']) - 1, color='r', linestyle='--', label='Start Fine-tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy (Cumulative)', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.grid(True)\n\n# Subplot 2: Loss\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.axvline(x=len(history_initial.history['loss']) - 1, color='r', linestyle='--', label='Start Fine-tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss (Cumulative)', fontsize=14)\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nCumulative Accuracy and Loss plots displayed.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Evaluating the fine-tuned model on the test set...\")\n\nloss, accuracy = model.evaluate(test_ds)\n\nprint(f\"\\nTest Loss (after fine-tuning): {loss:.4f}\")\nprint(f\"Test Accuracy (after fine-tuning): {accuracy:.4f}\")\n\nprint(\"\\nModel testing complete.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-05T10:10:53.507Z"}},"outputs":[],"execution_count":null}]}